# 作业1：后端开发与算法开发的角色

根据阿里云客服工作台文档，我们作为后端开发和算法开发需要承担以下工作：

## 一、后端开发需要做什么

### 1. 数据库设计

需要设计4个核心表：

**（1）类目表（Category）**
- 类目ID、父类目ID、类目名称、层级
- 支持无限级分类（一级类目、二级类目、多级类目）

**（2）FAQ表**
- FAQ ID、所属类目ID
- FAQ标题（标准问法）
- 答案内容
- 答案类型（纯文本/富文本/卡片）
- 生效时间、失效时间
- 标签ID、视角ID

**（3）相似提问表**
- 提问ID、关联FAQ ID
- 提问内容
- 每条FAQ最多200条相似问法

**（4）发布记录表**
- 记录ID、操作类型、操作内容
- 目标环境（测试/正式）

### 2. API接口开发

**类目管理API**
- 创建一级/二级/多级类目
- 修改类目名称
- 删除类目（需检查是否存在下级FAQ）
- 按类目名称检索

**FAQ管理API**
- 创建FAQ（选择类目、输入标题和答案）
- 编辑FAQ内容
- 转移FAQ到其他类目
- 查询FAQ列表（支持关键词搜索）
- 删除/失效FAQ
- 批量导入（最多3000条）、批量导出（最多50000条）

**发布中心API**
- 测试环境FAQ同步到正式环境
- 查看发布记录

### 3. 业务逻辑

**双环境隔离**
- 测试环境和正式环境分离
- 测试环境中修改FAQ后，需要通过"发布中心"发布才能同步到正式环境
- 正式环境中无法直接修改FAQ，必须切回测试环境

**生效时间判断**
- FAQ支持设置生效时间范围
- 超出或未达到时间时，知识自动失效
- 支持精确到年月日时分秒

**批量操作**
- 批量删除、批量导入、批量导出
- 基于FAQ列表做批量的转移、下载、发布、失效、删除

---

## 二、算法开发需要做什么

### 1. 使用BERT进行文本编码

使用bert-base-chinese预训练模型对FAQ标题和用户提问进行语义编码，输出768维向量表示。

**编码流程：**
```
输入：[CLS] FAQ标题/用户提问 [SEP]
         ↓
BERT模型（12层Transformer Encoder）
         ↓
输出：768维向量（取[CLS]位置的向量）
```

### 2. 相似度匹配

计算用户提问与FAQ标题/相似提问的余弦相似度，返回最相似的FAQ答案。

**匹配流程：**
```
用户输入问题 → BERT编码 → 向量检索 → 余弦相似度计算 → 返回答案
```

---

## 三、是否需要使用大模型？

**不需要。**

根据文档描述，这是一个"相似度匹配"场景，不是生成式问答。

**原因分析：**

1. **场景匹配**：文档明确说明"用户的提问直接与历史的提问进行相似度匹配"，这是典型的语义相似度计算任务，不是开放式问答。

2. **技术选型**：BERT预训练模型已经具备强大的语义理解能力，可以很好地完成相似度匹配任务。

3. **成本考虑**：大语言模型（如GPT、通义千问）推理成本高、响应慢，对于FAQ匹配场景是杀鸡用牛刀。

4. **维护成本**：接入大模型需要额外的API调用、提示词工程、输出控制，增加了系统复杂度。

**结论：**
- 使用BERT预训练模型即可满足当前需求
- 大语言模型可以作为后期优化选项（如用于答案生成、多轮对话等高级特性）
- 当前阶段专注于相似度匹配的准确率和响应速度
